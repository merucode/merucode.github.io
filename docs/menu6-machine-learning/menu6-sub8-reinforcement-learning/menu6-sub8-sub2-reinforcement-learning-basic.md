---
layout: default
title: Reinforcement Learning Basic
parent: Reinforcement Learning
grand_parent: Machine Leaning
nav_order: 2
---

# Reinforcement Learning Basic
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
<!------------------------------------ STEP ------------------------------------>

## STEP 1. MDP(Markov Decision Process)

### Step 1-1. MDP

* **MDP**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531014605568.png" alt="image-20230531014605568" style="zoom:80%;" />

* **Component**

  | Items | Description                   | Equation                                                     |
  | ----- | ----------------------------- | ------------------------------------------------------------ |
  | S     | Group of states               |                                                              |
  | A     | Group of actions              |                                                              |
  | P     | Transition probability matrix | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531015115476.png" alt="image-20230531015115476" style="zoom: 67%;" /> |
  | R     | Reward function               | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531015131196.png" alt="image-20230531015131196" style="zoom:67%;" /> |
  | Î³     | Damping factor                |                                                              |



### Step 1-2. Policy function and two value function

* **Policy function**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531015546314.png" alt="image-20230531015546314" style="zoom: 67%;" />

  * Policy function is related with agent(not environment)

* **If ğ… is given**, we can get **two value function**(two value function is depend on ğ…)

* **State value function**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531015856727.png" alt="image-20230531015856727" style="zoom: 67%;" /> 

* **State-action value function**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531020005369.png" alt="image-20230531020005369" style="zoom:67%;" />



### Step 1-3. Prediction, Control

* **Prediction** : evaluate value of state with given ğ…
* **Control** : find best policy function(ğ…)



<br>



<!------------------------------------ STEP ------------------------------------>

## STEP 2. Bellman Equation

### Step 2-1. Bellman Expectation Equation

* **Bellman Expectation Equation**(ğ… is given)

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531020529338.png" alt="image-20230531020529338" style="zoom:67%;" />

  * **Step 1 Example**

    | v                                                            | q                                                            |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531021438490.png" alt="image-20230531021438490" style="zoom: 67%;" /> | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531021557094.png" alt="image-20230531021557094" style="zoom:67%;" /> |
    | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531021512841.png" alt="image-20230531021512841" style="zoom:67%;" /> | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531021753802.png" alt="image-20230531021753802" style="zoom:67%;" /> |

    

  * What it means to **know MDP** is **knowing r<sub>S</sub><sup>a</sup>, P<sub>ss'</sub><sup>a</sup>**
    * **Know MDP** â†’  Step 2. equation â†’ Model-based, planning 
    * **Don't know MDP** â†’ Step 0. equation â†’ Model-free, sampling

### Step 2-2. Optimal Value/Policy

* **Optimal Value/Policy**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531022003525.png" alt="image-20230531022003525" style="zoom: 80%;" />

  * ìƒíƒœ ë³„(s, s' ...)ë¡œ ê°€ì¥ ë†’ì€  valueì˜ ì •ì±…ì˜ ë‹¤ë¥¸ ê²½ìš°

    â†’ ê°ê°ì˜ ì •ì±…(ğ…<sub>s</sub>, ğ…<sub>s'</sub> ...)ì„ ì¡°í•©í•´ ìƒˆë¡œìš´ ì •ì±…(ğ…<sub>*</sub>) ê°€ëŠ¥

  * MDPëŠ” ğ…<sub>*</sub>ê°€ ë°˜ë“œì‹œ ì¡´ì¬í•¨

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531022311783.png" alt="image-20230531022311783" style="zoom:67%;" />



### Step 2-3. Bellman **Optimality** Equation

* **Bellman Optimality Equation**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531022427461.png" alt="image-20230531022427461" style="zoom:67%;" />

  *  Reason that is not max operator in front of q(s,a) equation
    *   ìš°ë¦¬ê°€ í‰ê°€í•˜ë ¤ëŠ” aì— ëŒ€í•œ valueì—ì„œ aëŠ” í•­ìƒ ìµœì„ ì˜ í–‰ë™ì„ ì˜ë¯¸í•˜ì§€ëŠ” ì•Šê¸° ë•Œë¬¸ì— max ì—°ì‚°ìê°€ ì•ì— ë¶™ì§€ ì•ŠìŒ
  *  ğ…(a`|`s) â†’ max<sub>a</sub>
    * **Bellman Expectation Equation** : 2 stochastic factor(P, ğ…)
      * use to evaluate ğ…
    * **Bellman Optimality Equation** : 1 stochastic factor(P)
      * use to get best value

<br>



<!------------------------------------ STEP ------------------------------------>

## STEP 3. Know MDP, and Small Problem

### Step 3-1. Preview

* **Prediction : Iterative policy evaluation**

* **Control**

  1. **Policy iteration**
  2. **Value iteration**

* using **tabular method**(Example)

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531023414340.png" alt="image-20230531023414340" style="zoom:67%;" />



### Step 3-2. Prediction : Iterative Policy Evaluation(ğ… given)

* **Method**

  1. Initialize table

  2. Update one state

     <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531023832922.png" alt="image-20230531023832922" style="zoom:67%;" />

     * ë¬´ì˜ë¯¸í•œ ê°’ì— ì‹¤ì œ ê°’ì´ ì„ì—¬ ë°˜ë³µì— ì˜í•´ ì‹¤ì œ ê°’ì— ê°€ê¹Œì›Œ ì§

     * ex) ğ…(ë™ì„œë‚¨ë¶`|`s) = 0.25, r = -1, P = 1, ì´ˆê¸° value = 0

       <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531023934269.png" alt="image-20230531023934269" style="zoom:50%;" />

  3. apply 2. on all states

  4. iterate 2.~3.

     <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531024132739.png" alt="image-20230531024132739" style="zoom:50%;" />



### Step 3-3. Contrl : Policy iteration

* **ì •ì±… í‰ê°€/ì •ì±… ê°œì„  ë°˜ë³µ**

  * Policy evaluation : iterative policy evaluation
  * Policy improvement : get ğ…<sub>greedy</sub> from policy evaluation

* **Greedy Policy**

  * Act to get more good value
  *  ğ…<sub>greedy</sub> is improved over ğ…

* **early stopping**

  * There is no need to repeat to the limit, because even a **little repetition is worthwhile**

* **Method**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531024636710.png" alt="image-20230531024636710" style="zoom: 67%;" />



### Step 3-4. Control : Value iteration

* Get **optimal policy** from **optimal value** derived from **bellman optimality equation**

* **Method**

  1. Get optimal value From bellman optimality equation using iterative policy evaluation

     <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531025334910.png" alt="image-20230531025334910" style="zoom:67%;" />

     * ex)

       <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531025416567.png" alt="image-20230531025416567" style="zoom:50%;" />

  2. Get ğ…<sub>*</sub> from optimal value

     * ğ…<sub>*</sub> is greedy policy to optimal value

<br>



<!------------------------------------ STEP ------------------------------------>

## STEP 4. Don't Know MDP, and Small Problem : Prediction

### Step 4-1. Preview

* **Prediction**

  1. MonteCarlo Method
  2. Temporal difference

* **model(model of enviromnet)** : ì•¡ì…˜ì— ëŒ€í•˜ì—¬ í™˜ê²½ì´ ì–´ë–»ê²Œ ì‘ë‹µí• ì§€ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë“  ê²ƒ

* using **tabular method**(Example)

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531030203345.png" alt="image-20230531030203345" style="zoom:50%;" />

  * We know r=-1, P=1, but **system don't know about r, P**



### Step 4-2. MC(MonteCarlo) Learning

* Get value from **many sampling**

* **Method**

  1. Initialize table : (0, 0)

     * N(s) : counts of enter, V(s) : sum of returns

  2. Experience : arrived at S<sub>T</sub>

     <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531030039520.png" alt="image-20230531030039520" style="zoom:50%;" />

  3. Update table

     * N(s<sub>t</sub>) â† N(s<sub>t</sub>) + 1

     * V(s<sub>t</sub>) â† V(s<sub>t</sub>) + G<sub>t</sub>

       <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531030753721.png" alt="image-20230531030753721" style="zoom:50%;" />

  4. Calculate value

     <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531030900628.png" alt="image-20230531030900628" style="zoom:67%;" />

* **Version of partial update**

  * Above method is need **many episode to update v<sub>ğ…</sub>**(Because it need average)
    * Below expression is need only **one episode finishing to update  v<sub>ğ…</sub>**
    * Don't need to save N(s<sub>t</sub>)

  
  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531031043039.png" alt="image-20230531031043039" style="zoom:67%;" />
  
  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230531031114200.png" alt="image-20230531031114200" style="zoom:67%;" />
  



### Step 4-3. Implement MonteCalro Learning

* 4 things needed for implement 
  1. environment
  2. agent
  3. experience part
  4. learning part
* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch4_MCLearning.ipynb)



### Step 4-4. TD(Temporal Difference) Learning

* **MC and TD**

  | Items        | MC                                                           | TD                                                           |
  | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | Update point | One episode finish                                           | **After operate step**<br>Don't need to finish episode       |
  | Theory       | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021630218.png" alt="image-20230602021630218" style="zoom:67%;" /> | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021653149.png" alt="image-20230602021653149" style="zoom:67%;" /> |
  | Note         | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021714113.png" alt="image-20230602021714113" style="zoom:67%;" /> |                                                              |

* **TD target**
  * r<sub>t+1</sub> +Î³ v<sub>ğ…</sub>(s<sub>t+1</sub>) ì„ ì—¬ëŸ¬ë²ˆ sampling í•˜ì—¬ í‰ê· ì„ ë‚´ë©´ v<sub>ğ…</sub>(s<sub>t</sub>) ì— ìˆ˜ë ´
  * ì¦‰, **r<sub>t+1</sub> +Î³ v<sub>ğ…</sub>(s<sub>t+1</sub>)** ëŠ” ìš°ë¦¬ì˜ ëª©í‘œ(ì •ë‹µ)ê°€ ë˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— **TD target**

* **TD Learning Algorithm**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021856194.png" alt="image-20230602021856194" style="zoom:67%;" />

  * ex> s<sub>0</sub> â†’ s<sub>1</sub> â†’  s<sub>2</sub> â†’  ...  s<sub>11</sub> â†’  ì¢…ë£Œ

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021927693.png" alt="image-20230602021927693" style="zoom:50%;" />



### Step 4-5. Implement TD Leaning

* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch4_TDLearning.ipynb)



### Step 4-6. MC vs TD

* **Compare MC and TD**

  | Items    | MC                                                           | TD                                                           |
  | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | Leaning  | Episodic MDP<br>(ì¢…ë£Œìƒíƒœê°€ ìˆëŠ” ê²ƒ)                         | Episodic MDP<br>Non-Episodic MDP                             |
  | Bias     | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021630218.png" alt="image-20230602021630218" style="zoom:67%;" /><br>**unbiased** | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602021653149.png" alt="image-20230602021653149" style="zoom:67%;" /><br>**biased** |
  | Variance | ì„œìš¸ì‹œì²­ â†’ ê°•ë¦‰<br>ë³€ë™ì„± í¼(ì‘ì€ **Î±**)                     | ì„œìš¸ì‹œì²­ â†’ ì• í¸ì˜ì <br>ë³€ë™ì„± ë‚®ìŒ(í° **Î±**)                |

* **Reason that TD is biased**

  * **r<sub>t+1</sub> +Î³ v<sub>ğ…</sub>(s<sub>t+1</sub>)**(ì‹¤ì œ TD target) is **unbiased**

  * **r<sub>t+1</sub> +Î³ V(s<sub>t+1</sub>)**(ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” TD target) is **baised**

    * TDì—ì„œëŠ” ì‹¤ì œ ê°’(**v<sub>ğ…</sub>**)ëª¨ë¦„ìœ¼ë¡œ ì¶”ì¸¡ ê°’(**V**)ì„ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ update ìˆ˜í–‰

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602024721754.png" alt="image-20230602024721754" style="zoom: 67%;" />

â€‹	

### Step 4-7. n Step MDP

* **Relation between MC and TD**

  * N step TD target

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602024943233.png" alt="image-20230602024943233" style="zoom:67%;" />

  * N = T(end point) â†’ MC

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602025105313.png" alt="image-20230602025105313" style="zoom: 67%;" />



<br>



<!------------------------------------ STEP ------------------------------------>

### ì´ˆì•ˆ ì‘ì„±

## STEP 5. Don't Know MDP, and Small Problem : Control

### Step 5-1. Preview

* **Control**
  1. MC Control
  2. TD Control : SARSA
  3. Q Leaning



### Step 5-2. MC Control

* We want to use **Policy interation** but **don't know MDP(r<sub>S</sub><sup>a</sup>, P<sub>ss'</sub><sup>a</sup>)**

  * don't know **r<sub>S</sub><sup>a</sup>**  â†’ don't select ğ…<sub>greedy</sub>
  * don't know **P<sub>ss'</sub><sup>a</sup>** â†’ don't select action(don't know result of action)

* **Solution**

  1. Policy evaluation : MC Learning

  2. Policy improvement : use Q instead of V

     | V                                                            | Q                                                            |
     | ------------------------------------------------------------ | ------------------------------------------------------------ |
     | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602032625474.png" alt="image-20230602032625474" style="zoom:67%;" /> | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602032545261.png" alt="image-20230602032545261" style="zoom:67%;" /> |
     |                                                              | Need q(s,a) value to use MC                                  |

  3. exploration

     * ë¬´ì¡°ê±´ í•œ ë°©í–¥ë§Œ Action ì‹œ ë” ì¢‹ì€ valueë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ

     * Introduce **Îµ-greed**(decay)

       <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602032851236.png" alt="image-20230602032851236" style="zoom:80%;" />

       * decay : ì´ˆê¸° ë†’ì€ Îµ, í›„ê¸° ë‚®ì€ ì¼ì • Îµ

* **MC Control**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602032958062.png" alt="image-20230602032958062" style="zoom:67%;" />



### Step 5-3. Implement MC Control

* **Example**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602033510256.png" alt="image-20230602033510256" style="zoom:67%;" />

* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch5_MCContorl.ipynb)



### Step 5-4. TD Control : SARSA

* **Policy evaluation : using TD instead of MD**

  * It's help to **update not end of episode but end of step**

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602034929327.png" alt="image-20230602034929327" style="zoom:67%;" />

* **SARSA**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602035015216.png" alt="image-20230602035015216" style="zoom:67%;" />

  * **TD Target**

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602035151685.png" alt="image-20230602035151685" style="zoom:80%;" />

    * **ê¸°ëŒ€ê°’ ì•ˆì˜ ìˆ˜ì‹ Sampling â†’ ì‹¤ì œ ê¸°ëŒ€ê°’ì— ê°€ê¹Œì›Œì§**

  * **SARSA**

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602035213985.png" alt="image-20230602035213985" style="zoom:80%;" />



### Step 5-5. Implement TD Contorl : SARSA

* [code_url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch5_SARSA.ipynb)



### Step 5-6. TD Contorl : Q Learning

* **On/Off Policy**
  * on-policy : same **target policy** and **behavior policy**
  * off-policy : different **target policy** and **behavior policy**

* **Target/Behavior Policy**

  * target policy : ê°•í™”í•˜ê³ ì í•˜ëŠ” ëª©í‘œê°€ ë˜ëŠ” ì •ì±…
  * behavior policy : ì‹¤ì œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì—¬ ê²½í—˜ì„ ìŒ“ëŠ” ì •ì±…

* **Off Policy Adventage**

  1. reuse past experience
  2. learning from data of person
     * system learn from (s, a, r, s') data
  3. 1:N or N:1 learning is possible

* **Theory Background**

  - From bellman optimality equation

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602040912800.png" alt="image-20230602040912800" style="zoom: 67%;" />

    - If we know **q<sub>*</sub>**, optimal policy is below(just move to highest **q<sub>*</sub> action**)

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602041116527.png" alt="image-20230602041116527" style="zoom:67%;" />

    * **Qì— ëŒ€í•˜ì—¬ greed policy**

  - So, our purpose is getting **q<sub>*</sub>**

    * From bellman optimality equation(step 0)

      <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602041418605.png" alt="image-20230602041418605" style="zoom:67%;" />

    * Replace E with sampling value

* **Compare SARSA and Q-Learning**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602041624145.png" alt="image-20230602041624145" style="zoom:67%;" />

  | Items           | SARSA                                                        | Q-Learning                                                   |
  | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | Behavior Policy | Qì— ëŒ€í•´<br>Îµ-greedy                                         | Qì— ëŒ€í•´<br/>Îµ-greedy                                        |
  | Target Policy   | Qì— ëŒ€í•´<br/>Îµ-greedy                                        | Qì— ëŒ€í•´<br/>greedy                                          |
  | Policy          | on                                                           | off<br>Difference Behavior and Target policy                 |
  | Theory          | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602042239508.png" alt="image-20230602042239508" style="zoom:67%;" /><br>Bellman Expectaion | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602042251835.png" alt="image-20230602042251835" style="zoom:67%;" /><Br>Bellman Optimality |
  | Difference      | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602042328635.png" alt="image-20230602042328635" style="zoom:80%;" /> | <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602042335110.png" alt="image-20230602042335110" style="zoom:80%;" /><br>ğ… only greedy select for Q |
  | Note            |                                                              | ğ…<sub>*</sub> is dependent on environment(q)                 |



### Step 5-6. Implement Q-Learning

* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch5_QLearning.ipynb)



<br>



<!------------------------------------ STEP ------------------------------------>

## STEP 6. Deep RL

### Step 6-1. Function to save data

* To solve large scale problem 
  * We use deep RL(Deep Learning + Reinforce Learning)
* Don't use tabular method to solve problem have to many state(ë°”ë‘‘, continuous state space problem ...)
  * Introduce function to save data
* **Function Generalization**
  * **Make general function for state/action data express using parameters(w or theta)**
  * Small storage to save results of learning
  * Use deep learning to find general function



### Step 6-2. Implement Function Generalization

* **Example**

  * get data from below function and make general function

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602044713357.png" alt="image-20230602044713357" style="zoom:67%;" />

* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch6_Fitting.ipynb)


<br>

##  ì—¬ê¸°ë¶€í„° ë¹ ë¥´ê²Œ ì§„í–‰ ë³µìŠµ í•„ìš”

<!------------------------------------ STEP ------------------------------------>

## STEP 7. Model free, Large state/action space

### Step 7-1 Preview

* **Deep ML**

  1. Value based : v<sub>ğ…</sub>(s), q<sub>ğ…</sub>(s,a) â†’ neural net
  2. Policy Based : ğ…(a`|`s) â†’ neural net

* **RL Agent**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602050605538.png" alt="image-20230602050605538" style="zoom:67%;" />

  1. value based
     * Select action from value(q(s,a))
     * SARSA, Q-Learning
     * When select action, **only greedy select high q(s,a)**
       * So, don't need ğ…(a`|`s) and **q(s,a)** is rule of ğ…
  2. policy based
     * Select action from ğ…(a`|`s)
     * When select action, **only use ğ…**. So, don't need value, evaluation function
  3.  Actor-Critic
     * Select action from both value and policy
       * Actor : ğ…
       * Critic : v(s) or q(s,a)

* **value based**
  * ğ…ê°€ ê³ ì •ë˜ì—ˆì„ ë–„, ğ…ì˜ ê°€ì¹˜í•¨ìˆ˜ v<sub>ğ…</sub>(s)ë¥¼ í•™ìŠµ
  * value network
    * Î¸ is neural net parameters
    * purpose : learning proper Î¸, to v<sub>Î¸</sub>(s) get proper value per states



## Step 7-2. Learn Value Network

* Value network of v<sub>Î¸</sub>(s)(ğ… ê³ ì •)

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602051537290.png" alt="image-20230602051537290" style="zoom:50%;" />

* **Loss function**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602051650244.png" alt="image-20230602051650244" style="zoom:67%;" />

  * ìœ„ ì‹ì€, ì–´ë–¤ sì— ëŒ€í•œ ê²ƒì¸ì§€ê°€ ì—†ìŒ

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602051750947.png" alt="image-20230602051750947" style="zoom:67%;" />

  * ğ…ì— ì˜í•œ sampling í†µí•´ ê¸°ëŒ€ê°’ì„ ê·¼ì‚¬ì ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥, 

  * gradient ìˆ˜í–‰í•˜ë©´(ìƒìˆ˜ ìƒëµ)

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602051915991.png" alt="image-20230602051915991" style="zoom:67%;" />

  * ğ…ì— ì˜í•´ ìƒíƒœ sì— ë“¤ì–´ê°ˆ ê²½ìš°

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052018759.png" alt="image-20230602052018759" style="zoom:67%;" />

  * ìœ„ ì‹ í•˜ë‚˜ë¡œë§Œì€ ì„±ë¦½í•˜ì§€ ì•Šìœ¼ë‚˜, Samplingì„ í†µí•´ ìˆ˜ë§ì€ ê°’ìœ¼ë¡œ ìš°ë³€ í‰ê· ì„ ë‚´ë©´ ì¢Œë³€ì— ê·¼ì‚¬í•˜ê²Œ ë¨

  * Î¸ Update

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052149815.png" alt="image-20230602052149815" style="zoom:67%;" />

  * Sampling ë° gradient descent ì˜í•´ v<sub>Î¸</sub>(s)ëŠ” v<sub>true</sub>(s)ì— ê·¼ì‚¬í•˜ê²Œ ë¨

* **But, we don't know  v<sub>true</sub>(s)**

  * To get  v<sub>true</sub>(s)
    1. MC return
    2. TD target



### Step 7-3 MC Return

* **MC return**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052521771.png" alt="image-20230602052521771" style="zoom:67%;" />

  * G<sub>t</sub> ì‚¬ìš© ê°€ëŠ¥í•œ ì´ìœ ëŠ” ì‹¤ì œ ê°€ì¹˜í•¨ìˆ˜ ì •ì˜ê°€ G<sub>t</sub>ì˜ ê¸°ëŒ€ê°’ì´ê¸° ë•Œë¬¸ì—(G<sub>t</sub> = v<sub>ture</sub>(s<sub>t</sub>))

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052542477.png" alt="image-20230602052542477" style="zoom:67%;" />

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052554342.png" alt="image-20230602052554342" style="zoom:67%;" />



### Step 7-4. TD Target

* Introduce TD target instead of G<sub>t</sub>

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052845953.png" alt="image-20230602052845953" style="zoom:67%;" />

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602052857615.png" alt="image-20230602052857615" style="zoom:67%;" />

* TD target is ìƒìˆ˜ not function of Î¸ â†’ Î¸ í¸ë¯¸ë¶„ì‹œ TD target í•­ 0
  * ë§Œì•½ ìƒìˆ˜ ì·¨ê¸‰ ì•ˆí•œë‹¤ë©´ TD target(ì¶”ì¸¡ ì •ë‹µê°’)ë„ ë³€í•˜ê²Œ ë˜ì–´ ëª¨ë¸ ì•ˆì „ì„±ì´ ë–¨ì–´ì§



### Step 7-5. Deep Q learning

* Value based agent don't have **explicit policy(ğ…)**
  * ğ… is not exist
  * act for greedy at q(s,a) â†’ **implicit policy q(s,a)**

* **Theory Background**

  * From Bellman optimality Equation

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602053402193.png" alt="image-20230602053402193" style="zoom:67%;" />

    * ì¶”ì¸¡ ì •ë‹µì¸ TD target ê³¼ ì¶”ì¸¡ì¸ Q(s,a) ì‚¬ì´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸

  * Loss function

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602053505409.png" alt="image-20230602053505409" style="zoom:67%;" />

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602053534497.png" alt="image-20230602053534497" style="zoom:67%;" />

    * ì •ë‹µ Q<sub>*</sub>ê³¼ Q<sub>Î¸</sub> ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸

  * use mini-batch instead of E

* **Deep Q Learning : pseudo code**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602053818494.png" alt="image-20230602053818494" style="zoom:67%;" />

  * 3-A real act(Behavior policy : eps-greedy)

  * 3-C is not real action but operate to calculate TD value(Target policy : greedy)

  * off-policy

  * When Implement we just define L(Î¸). don't need gradient of L(Î¸)

    * optimizer and ì—­ì „íŒŒê°€ ì•Œì•„ì„œ ê³„ì‚° ìˆ˜í–‰í•´ì¤Œ

      <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602054440045.png" alt="image-20230602054440045" style="zoom:67%;" />



### Step 7-6. Implement DQN

* **Experience Replay**

  * episode is consist of many transitions(one transition = e<sub>t</sub>)

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602054718490.png" alt="image-20230602054718490" style="zoom:67%;" />

  * use replay buffer to **reuse transition**

    * If save new data, delete oldest data

    * mini-batch extract data from replay buffer

      * mini-batch is consist of not continuous data. It's mean they are small correalation

        â†’ help to improve performance

  * only use to off-policy algorithm

* **Target Network**

  <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602055229039.png" alt="image-20230602055229039" style="zoom:80%;" />

  * Q-Learning ì •ë‹µ

    <img src="./../../../images/menu6-sub8-sub2-reinforcement-learning-basic/image-20230602055312486.png" alt="image-20230602055312486" style="zoom:80%;" />

    * Î¸ì— ëŒ€í•œ í•¨ìˆ˜ì´ë¯€ë¡œ Î¸ê°€ ë³€í•˜ë©´ ë³€í•˜ê²Œ ë¨
    * ì¼ì • ì‹œê°„ freezing ì‹œí‚¤ëŠ” Î¸<sub>i</sub><sup>-</sup> ë„ì…

  * **Target and Q network**

    * Target network : Calculate answer, freezing Î¸<sub>i</sub><sup>-</sup>
    * Q network :  Learning, Î¸ updated, ì¼ì • ì£¼ê¸°ë§ˆë‹¤ Î¸ â†’ Î¸<sub>i</sub><sup>-</sup>


* [code url](https://github.com/merucode/study_ML/blob/master/RL/basic/ch7_DQN.ipynb)


<br>



<!------------------------------------ STEP ------------------------------------>



https://github.com/seungeunrho/RLfrombasics/blob/master/ch8_DQN.py

<br>



<!------------------------------------ STEP ------------------------------------>





<br>



<!------------------------------------ STEP ------------------------------------>





