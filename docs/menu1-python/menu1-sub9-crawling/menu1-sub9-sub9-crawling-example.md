---
layout: default
title: Crawling Example
parent: Crawling
grand_parent: Python
nav_order: 1
---

# Crawling Example
{: .no_toc }



## Step 1. Naver ì¢…ëª©í† ë¡ ì‹¤ ê²Œì‹œê¸€ Crawling

### Step 1-1. File Structure

    ```
    .
    â”œâ”€â”€ ğŸ“app
    â”‚Â Â  â”œâ”€â”€ Dockerfile
    â”‚Â Â  â”œâ”€â”€ main.py
    â”‚Â Â  â”œâ”€â”€ requirements.txt
    â”‚Â Â  â””â”€â”€ ğŸ“src
    â”‚Â Â      â”œâ”€â”€ connect_tcp.py
    â”‚Â Â      â”œâ”€â”€ naver_stock_post.py
    â”‚Â Â      â”œâ”€â”€ nlp.py
    â”‚Â Â      â””â”€â”€ try_request.py
    â”œâ”€â”€ docker-compose.prod.yml
    â””â”€â”€ docker-compose.yml
    ```

### Step 1-2. Docker 

* `docker-compose.yml`

    ```Docker
    version: '3.7'

    services:
    app:
        container_name: myapp
        build:
        context: ./app/
        dockerfile: Dockerfile.dev
        volumes:
        - ${PWD}/app/:/usr/src/app
        env_file:
        - ./.env  # For DB
        ports:
        - 80:80
        command: /bin/bash   # To be possible : docker exec -it myapp /bin/bash
        stdin_open: true     # ..
        tty: true            # ..
    ```

* `docker-compose.prod.yml`

    ```Docker
    version: '3.7'

    services:
    app:
        container_name: myapp
        build:
        context: ./app/
        dockerfile: Dockerfile.dev
        volumes:
        - ${PWD}/app/:/usr/src/app
        env_file:
        - ./.env
        ports:
        - 8080:80
        command: python -u main.py  # -u option : Show python logs
    ```

* `app/Dockerfile`

    ```Docker
    FROM python:3.9

    WORKDIR /usr/src/app

    RUN pip install --upgrade pip
    COPY ./requirements.txt .
    RUN pip install -r requirements.txt
    ```

### Step 1-3. Code 

* `app/main.py`
    * ì¼ì • ì‹œê°„ë§ˆë‹¤ í¬ë¡¤ë§ ìš”ì²­

    ```python
    import time
    from src.try_request import request_crawling

    while True:
        request_crawling()
        time.sleep(36000) # Wait 10 hour before sending the next request
    ```

* `app/src/try_request`
    * í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ìš”ì²­
    * ê³¼ë„í•œ í¬ë¡¤ë§ ìš”ì²­ ì‹œ ìš”ì²­ ì„œë²„ì—ì„œ ë§‰ëŠ” ê²½ìš° ì²˜ë¦¬

    ```python
    import time
    import requests
    from src.post import  get_word_counts_from_post

    codes = [...]

    def request_crawling():
        try:
            for code in codes:
                get_word_counts_from_post(code) 
        except requests.exceptions.RequestException as e:
            print(f"An error occurred: {e}")
            time.sleep(3600) # Wait 1 hour and try again
            get_word_counts_from_post(code)
        except Exception as e:
            print(f"unexpected error occurred: {e}")
            time.sleep(3600) # Wait 1 hour and try again
            get_word_counts_from_post(code)
        pass
    ```

* `app/src/post.py`
    * í¬ë¡¤ë§ ì½”ë“œ

    ```python
    import pandas as pd
    import time
    import datetime
    from datetime import timedelta
    from collections import Counter

    from bs4 import BeautifulSoup
    import requests

    from src.connect_tcp import connect_tcp_socket
    from src.nlp import collect_nouns

    ### Ready for crawling
    headers = {
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
        "referer":"https://finance.naver.com/item/board.naver?"  
        }
    # 100 page ì´ìƒ ë¶€í„° Referrer-Policy : unsafe-url ê´€ë ¨ referer ì¶”ê°€
    engine = connect_tcp_socket() # DB connect engine ìƒì„±
    stock_name_dict = {...}

    ### Main control code
    def get_word_counts_from_post(code):
        stock_name = stock_name_dict[f'{code}']
        url = f"https://finance.naver.com/item/board.naver?code={code}&page=" # í¬ë¡¤ë§ URL ì„¤ì •

        ## Get post crawling from website(post date, title, views) as df
        df_posts = get_df_post(code, url)

        ## daily_divid_posts and return daily word_counts
        df_nlp = daily_divid_posts_nlp(df_posts, stock_name)
        df_nlp['code'] = code       # Add stock code
    
        ## Save nlp dataframe
        with engine.connect() as conn:
            df_nlp.to_sql('test', con=conn, if_exists='append',index=True)
        pass


    ### Conver post to dataframe and Handling initalize or update
    def get_df_post(code, url) -> pd.DataFrame:
        latest_date = get_latest_date_update(code) # Get latest data from DB

        if not latest_date: # DBì— ë°ì´í„°ê°€ ì—†ë‹¤ë©´ Initialize
            page_last = get_last_page_init(url)
            df_posts = post_to_df_init(page_last, url)
        else:               # DBì— ë°ì´í„°ê°€ ìˆë‹¤ë©´ Updaet
            df_posts = post_to_df_update(latest_date, url)
        return df_posts


    ### Get latest date from DB
    def get_latest_date_update(code) -> int:
        try:
            with engine.connect() as conn:
                df_check_data = pd.read_sql(f"SELECT * FROM test WHERE code='{code}' ORD
    ER BY date DESC LIMIT 1", con=conn)
            latest_date = df_check_data.iloc[-1, 0] 
            return latest_date
        except:
            return False


    ### Get last page from post
    def get_last_page_init(url) -> int:
        res = requests.get(url+'1', headers=headers)
        soup = BeautifulSoup(res.text, "lxml")
        page_last = int(soup.find("td", attrs={"class":"pgRR"}).find("a")["href"].split(
    '=')[-1])
        return page_last


    ### Convert post to dataFrame as initialize
    def post_to_df_init(page_last, url) -> pd.DataFrame:
        for i in range(page_last, 0, -1):   # ê°€ì¥ ë§ˆì§€ë§‰ í˜ì´ì§€ â†’ ìµœê·¼ í˜ì´ì§€ ìˆœ
            url_page = url + str(i)
            res = requests.get(url_page, headers=headers)
            
            soup = BeautifulSoup(res.text, 'html.parser')
            table = soup.find('table', {'class': 'type2'})
            df = pd.read_html(str(table))[0]
            df = df[['ë‚ ì§œ','ì œëª©']]        # Set extract columns
            df.columns = ['date', 'title']
            df['title'] = df['title'].str.split('[').str.get(0)   # Remove number of com
    ments[]
            df.dropna(inplace=True)         # Drop don't need rows
            df = df.iloc[::-1]              # Reserve df
            
            try:    # Merge extract df
                df_posts = pd.concat([df_posts,df], ignore_index=True)
            except: # Prevent code from stopping(first time concat occur error)
                df_posts = df

            time.sleep(10)                          # Time sleep for avoid error(over requests)
        return df_posts


    ### Convert post to dataframe as update
    def post_to_df_update(latest_date, url) -> pd.DataFrame:
        # ê¸°ì¡´ latest_dateëŠ” DB ìƒ ìµœê·¼ ë‚ ì§œì´ë¯€ë¡œ ì¤‘ì²© ë°©ì§€ë¥¼ ìœ„í•´ +1ì¼ ì¶”ê°€
        # DBì— 6ì›”1ì¼ê¹Œì§€ ë°ì´í„°ê°€ ìˆë‹¤ë©´, ìµœê·¼ ë¶€í„° 6ì›”2ì¼ 00:00:00 ë°ì´í„°ê¹Œì§€ ìˆ˜ì§‘
        latest_date = datetime.datetime.strptime(latest_date, '%Y-%m-%d') + timedelta(da
    ys=1) 
        for i in range(1, 1000000):  # ìµœê·¼ í˜ì´ì§€ â†’ ì˜ˆì „ í˜ì´ì§€ ìˆœ
            url_page = url + str(i)
            res = requests.get(url_page, headers=headers)

            soup = BeautifulSoup(res.text, 'html.parser')
            table = soup.find('table', {'class': 'type2'})
            df = pd.read_html(str(table))[0]
            df = df[['ë‚ ì§œ','ì œëª©']]        # Set extract columns
            df.columns = ['date', 'title']
            df['title'] = df['title'].str.split('[').str.get(0)   # Remove number of comments[]
            df.dropna(inplace=True)         # Drop don't need rows

            df['date'] = pd.to_datetime(df['date'])

            df = df[df['date'] > latest_date] # latest dateë³´ë‹¤ ìµœì‹  ë‚ ì§œë§Œ ë°˜ì˜

            # latest date ë³´ë‹¤ ìµœì‹  ë°ì´í„°ê°€ ì—†ë‹¤ë©´(ë¹ˆ df) ë” ì´ìƒ ë°›ì„ ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ return ì²˜ë¦¬
            if df.empty:       
                df_posts = df_posts.iloc[::-1]              # Reserve df
                return df_posts

            try:    # Merge extract df
                df_posts = pd.concat([df_posts,df], ignore_index=True)
            except: # Prevent code from stopping(first time concat occur error)
                df_posts = df

            time.sleep(10)                 # Time sleep for avoid error(over requests)
        pass

    ### Convert Post dataframe to NLP dataframe 
    def daily_divid_posts_nlp(df_posts, stock_name):
        df_posts['date'] = pd.to_datetime(df_posts['date'])
        df_posts['date'] = df_posts['date'].dt.strftime('%Y-%m-%d') # ë¹„êµìš© ë¬¸ìì—´ ì „í™˜
        now = datetime.datetime.now()
        # ì˜¤ëŠ˜ì´ 6ì›”2ì¼ì´ë¼ë©´ 6ì›”1ì¼ ë°ì´í„°ê¹Œì§€ ì „ì²˜ë¦¬ ìˆ˜í–‰
        end_date = now - timedelta(days=1)
        start_date = datetime.datetime.strptime(df_posts.iloc[0, 0], "%Y-%m-%d")

        col_name = ['date', 'words_count']
        df_collect_words_count = pd.DataFrame(columns=col_name)
        
        while start_date <= end_date:
            date = start_date.strftime("%Y-%m-%d")          # ë¹„êµìš© ë¬¸ìì—´ ì „í™˜
            df_post = df_posts.loc[df_posts['date']==date]  # daily post  
            pharse_list = df_post['title'].tolist()         # + df_post['content'].tolist()

            nouns = collect_nouns(pharse_list, stock_name)  # NPL preprocessing
            count = Counter(nouns)      # ë‹¨ì–´ ì¹´ìš´í„° ì²˜ë¦¬
            if len(count) != 0:
                lst_df_input = [date] + [count.most_common()]   # input data
                df_collect_words_count.loc[len(df_collect_words_count)] = lst_df_input

            # ### Step 6-2. Input value in dataframe
            start_date += timedelta(days=1) 

        return df_collect_words_count
    ```

* `app/src/nlp.py`
    * ë¬¸ì¥ â†’ ë‹¨ì–´ ì²˜ë¦¬

    ```python
    from konlpy.tag import Okt 
    from nltk import Text 
    from collections import Counter 

    my_stopwords_set = {...} # Set customized stopword(ë¶ˆìš©ì–´ ì„¸íŠ¸) 
    my_synonym_set = {'ê¹€ì˜ìˆ˜':('ì˜ìˆ˜','ì˜ìˆ˜ì”¨'), ...} # Set customized synonym(ë™ì˜ì–´ ì„¸íŠ¸) 
    
    Okt = Okt() 
    
    def collect_nouns(sen_list, stock_name): 
        nouns=[] 
        for k in sen_list: 
            noun = Okt.nouns(k)             # tokenized
            noun = clean_by_len(noun, 1)    # ë‹¨ì–´ ê¸¸ì´ 
            noun = clean_by_synonym(noun, my_synonym_set) # ë™ì˜ì–´
            noun = clean_by_stopwords(noun, my_stopwords_set) # ë¶ˆìš©ì–´
            nouns.extend(noun) 
        nouns = clean_by_freq(nouns, 1) # ë“±ì¥ ë¹ˆë„
        return nouns

    ### ë“±ì¥ ë¹ˆë„ cleaning  
    def clean_by_freq(tokenized_words, cut_off_count):  
        vocab = Counter(tokenized_words) # print(f'vocab : {vocab}') 
        uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}
        cleaned_words = [word for word in tokenized_words if word not in uncommon_words]  
        return cleaned_words  
        
    ### ë‹¨ì–´ ê¸¸ì´ cleaning 
    def clean_by_len(tokenized_words, cut_off_length):  
        cleaned_by_freq_len = []  
        for word in tokenized_words:  
            if len(word) > cut_off_length:  
                cleaned_by_freq_len.append(word)  
        return cleaned_by_freq_len 
    
    ### ë¶ˆìš©ì–´ ì²˜ë¦¬    
        def clean_by_stopwords(tokenized_words, stop_words_set):  
        cleaned_words = [] 
        for word in tokenized_words:  
            if word not in stop_words_set:  
                cleaned_words.append(word)  
        return cleaned_words 
    
    ### ë™ì˜ì–´ ì²˜ë¦¬ 
    def clean_by_synonym(tokenized_words, my_synonym_set): 
        cleaned_words =[] 
        for word in tokenized_words: 
            for key, set_tuple in my_synonym_set.items(): 
                if word in set_tuple: 
                    word = key 
            cleaned_words.append(word) 
        return cleaned_words
    ```

* `app/src/connect_tcp.py`
    * ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° pool ìƒì„±

    ```python
    import os

    import sqlalchemy
    import pg8000

    def connect_tcp_socket() -> sqlalchemy.engine.base.Engine:
        db_host = os.environ["INSTANCE_HOST"]  # Read ENV file in Docker compose
        db_user = os.environ["DB_USER"]  
        db_pass = os.environ["DB_PASS"]
        db_name = os.environ["DB_NAME"] 
        db_port = os.environ["DB_PORT"]

        connect_args = {}
        pool = sqlalchemy.create_engine(
            sqlalchemy.engine.url.URL.create(
                drivername="postgresql+pg8000",
                username=db_user,
                password=db_pass,
                host=db_host,
                port=db_port,
                database=db_name,
            ),
        )
        return pool
    ```